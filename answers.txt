We would like to train a model to predict the hammer_price of lots at upcoming auctions (some upcoming lots are included in your dataset). Note that this means that you can't use future data to predict the past. You may use as many or as few features as you like except for buyers_premium (it's a function of the final sale price. See SCHEMA.md for more details). This model should be optimized to minimize the relative error of each lot. Good solutions will have a MAPE below 30%.

- Did you perform any data cleaning, filtering or transformations to improve the model fit?
  Yes.  I filtered out the following features:
exluded features:
 artist nationalities: 2 of the 3 is training sets are american artist, this feature will more likely create bias. It also has a strong correlation to the artist's name feature
  
 artist birth year: It is strongly correclate with artist's death year and death year is a a feature that is already included in the training data

auction_sale_id: each sale id is unique, not a great training feature

lot_id: The data point is highly correlated to lot_place_in_auction, which is already captured

lot_description: skip for now, but could potentially use an nlp clustering algorithms to explore key word/hammer price correlation

lot_link: not relevant to the training model

work_title: same as description, no discernable insight for the corrent model

work dimentions: features being captured in work_width and work_height

buyer_premium: ignored per instruction

lot_place_in_auction and auction_lot_count:  Was included in V1, model performed much better without them.  They attributed to overfitting.

I also performed data cleaning to ignore rows with missing or incorrect training features.
I converted the categorical features using one-hot encoding
I parsed the auction dates into feature, year and month
I derived a feature gap_year, defined as auction_year - artist_death_year


- Why did you choose this model?

  I started out using simple feedforward neuro netowrk model; tried out MSE, MAPE, MSE and MSLE loss functions with ADAM, SGD and RMSProp respectively for each of the loss function.  

  I subsequently tried the simpleRNN model using the same set of loss functions and optimizers and well as a combination of the simpleRNN and feedforward neuro network but was not able to substantially improve the model error metrics compare to the simple feed forward model with 2 hidden layers.

  I also used ReLu activation function for its simplicity, computational efficiency and its non-linearity.  I used 2 hidden layers because that seemed to work best for this model after various trial and error.  Additional hidden layers started to yield overfitting behavior.  I have also tried leakyrelu to compensate for Relu's asymetrical shortcomings but it didn't yield any substantially better result.  Dropout layers reduced the overfitting behavior but also increased the error metrics as well.

  At the end of the day, out of everyting I tried, the simplest model yielded the best result with MAPE and the validation both hovering around 28-30%.


- What loss function did you use? Why did you pick this loss function?
 For the first version, I used MAPE loss function to hoping to minimize the relative error of each auction.  I also used MAE with additional hidden layers and dropouts.  It yielded similar results.  I also various iteration of MSE and mean_squared_logarithmic_error as loss functions, but none of the set up I tried yielded better results than MAPE.


- Describe how your model is validated.
  I split the training data into 95/5. 95% for model training and 5% for validation to test for overfitting.  I used 95% of data for training instead of traditional 80% due to limited traning data I have after data cleaning.  


- What error metrics did you evaluate your model against?
  The metric is also MAPE so we care about minizing the relative error of each lot.  I also looked at MAE as a secondary s error metrics 


- Generate and report a histogram of (hammer_price - predicted_hammer_price) / estimate_low (normalized model residuals). What are the mean and median values of this distribution?
  Report uploaded


 2A
We've been tasked to predict the hammer_price of auction lots without estimates (estimate_low, estimate_high). Assume that at any point in time, we have access to historical estimates of lots that have already sold but not to estimates of future lots we would like to predict. Repeat part 1. Briefly discuss your approach. Good solutions will have comparable accuracy to your work in part 1. Present your work as a runnable python (.py) file.

Because we have access to the historical estimate_low and estimate_high data, I used applied the same model we used for part 1 to predict estimate low price and subsequently estimate high price.  So for the test data without estimated prices, I use the estimate low and estimiate high model to predict the estimated low and high prices, then feed the test data with the predicted estimated features into our original model.  This approach yields a comparable accuracy to the results in part 1.



Please answer each in a few sentences.

Which features are most important for your model? Are there any features that surprised you? Given more data, describe other features you feel would be useful in improving the accuracy of your model.

Estimate Low and Estimate High were definitely by far the most important features for the model.  It cut the error rate by 50% when including these 2 feastures.  To my surpise, the dimention of the art work did not have as big of an impact on on the prediction model as I would have expected.  Given more data, especailly with art work from other artists, I do believe the nationalities and artist death year would be a more signifant feature in predicting the hammer price.




Now, assume we care much more about not over-predicting hammer_price than we do about under-predicting the hammer_price. Describe how you would go about changing your solution in terms of the model, objective function, etc.
If this is the case, I would design an asymetric loss function that penalize over prediction much more than under prediction.  One such loss function comes to mind is a quantile regression loss function.




Given more time but no new features, how much do you think you could improve the accuracy of your models by? Why? Now assume that we structure all of the information in the observable universe. What types of new features do you expect to have the greatest impact in performance?

Given more time but no new features, I suspect a categorical feature derived from a nlp clustering model of the description can potentially improve the accuracy of the model.



Was this fun? Which sections / questions were the most difficult and which were the easiest? 
Very much so!  I had a great deal of fun trying out various combination of models and features sets to improve the model.